{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "# The main module within keras is the \"Sequential\" model (but not the only module!)\n",
    "# This is the one we'll be concerned with most often.  The Dense and Activation are part of the network\n",
    "# layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print 'Training set', train_dataset.shape, train_labels.shape\n",
    "  print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "  print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings (See https://en.wikipedia.org/wiki/One-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print 'Training set', train_dataset.shape, train_labels.shape\n",
    "print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using stochastic gradient descent.\n",
    "\n",
    "Keras works like this:\n",
    "* You build a model, like this simple model:\n",
    "\n",
    "      model = Sequential([\n",
    "        Dense(32, input_dim=784),\n",
    "        Activation('relu'),\n",
    "        Dense(10),\n",
    "        Activation('softmax'),\n",
    "        ])\n",
    "\n",
    "* Then you define specifics of the model, such as optimize:\n",
    "\n",
    "\n",
    "        model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "Refer to the Keras page for more information.  A good place to start is here:  https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "Let's build the model to mimic the linear regression you did the last exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's subset the data for faster turnaround.\n",
    "train_subset = 5000\n",
    "\n",
    "# Let's build our model . . . \n",
    "model = Sequential()\n",
    "\n",
    "# The first argument is the type of layer to add; in this case \"Dense\" network, which is a typical \n",
    "# (weights * input + bias) configuration.  It's first input is the number of neurons to add.  In this case, we want\n",
    "# to add our 28 x 28 size, so there is a neuron for each input.  The next is the input dimension. \n",
    "# The init argument defines how the weights will be initalized.  In this case \"normal\" means that they will be \n",
    "# initalized with a normal gaussian (N(0, 1)).  Finally, the activation function is the relu function.\n",
    "model.add(Dense(train_dataset.shape[1], input_dim=train_dataset.shape[1], init='normal', activation='relu'))\n",
    "\n",
    "# The next layer for the Linear Regression model we're building is the output layer, which has the 1-hot encodings\n",
    "# for each of our classes (where, \"A, B, C, D, E, F, G, H, I, J\" represents the ten classes).  Same arguments for this\n",
    "# layer, expect for the activition, which should be \"softmax\", which provides the sigmodial like function to make\n",
    "# the 1-hot encoding label at the end of the network.\n",
    "model.add(Dense(train_labels.shape[1], init='normal', activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's define the optimizer (the stochastic gradient descent [SGD]) and the loss, compile, and then run.  We'll use \"categorical crossentropy\" for the loss, which will calculate the loss using crossentropy and our 1-hot encoded categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "import time\n",
    "sgd = SGD(lr=0.5)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll keep it short, only running 32 epochs with a batch size of 32 for each iteration.  The epoch must be divisible by the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "5000/5000 [==============================] - 0s - loss: 1.0111 - acc: 0.7334     \n",
      "Epoch 2/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.6191 - acc: 0.8200     \n",
      "Epoch 3/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.4825 - acc: 0.8638     \n",
      "Epoch 4/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.3865 - acc: 0.8850     \n",
      "Epoch 5/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.3140 - acc: 0.9184     \n",
      "Epoch 6/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.2547 - acc: 0.9288     \n",
      "Epoch 7/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.3990 - acc: 0.9134     \n",
      "Epoch 8/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.2483 - acc: 0.9396     \n",
      "Epoch 9/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.1732 - acc: 0.9586     \n",
      "Epoch 10/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.1297 - acc: 0.9722     \n",
      "Epoch 11/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.1348 - acc: 0.9706     \n",
      "Epoch 12/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.1105 - acc: 0.9748     \n",
      "Epoch 13/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0799 - acc: 0.9790     \n",
      "Epoch 14/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.2201 - acc: 0.9554     \n",
      "Epoch 15/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.1310 - acc: 0.9708     \n",
      "Epoch 16/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0887 - acc: 0.9812     \n",
      "Epoch 17/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.1231 - acc: 0.9754     \n",
      "Epoch 18/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.1173 - acc: 0.9732     \n",
      "Epoch 19/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0891 - acc: 0.9836     \n",
      "Epoch 20/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0817 - acc: 0.9846     \n",
      "Epoch 21/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0488 - acc: 0.9890     \n",
      "Epoch 22/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0504 - acc: 0.9910     \n",
      "Epoch 23/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0335 - acc: 0.9940     \n",
      "Epoch 24/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0367 - acc: 0.9924     \n",
      "Epoch 25/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0387 - acc: 0.9932     \n",
      "Epoch 26/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0458 - acc: 0.9914     \n",
      "Epoch 27/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0436 - acc: 0.9916     \n",
      "Epoch 28/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0336 - acc: 0.9944     \n",
      "Epoch 29/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0310 - acc: 0.9950     \n",
      "Epoch 30/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0196 - acc: 0.9960     \n",
      "Epoch 31/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0340 - acc: 0.9918     \n",
      "Epoch 32/32\n",
      "5000/5000 [==============================] - 0s - loss: 0.0207 - acc: 0.9952     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13dea0e50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset[:train_subset, :], train_labels[:train_subset], nb_epoch=32, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18540/18724 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_dataset, test_labels, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric names (below) are the metrics that are automatically calculated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"loss\" is the score that is produced by the optimizer and \"acc\" is the accuracy of that score (that is, how many times did the algorithm get it right with the training data provided).  When we calculated score from \"score = model.evaluate...\" we are scoring the model against unseen data (that is, data not used for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.70455819109475015, 0.9012497285318487]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the score from the training is (likely) higher than the score from the test data.  This is because test-on-train always overestimates.  Our mulinomial regression model is performing pretty well, at about 89% accuracy.  That's not bad.  Let's try to improve it, while learning some new things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "\n",
    "Let's turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units (nn.relu()) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build our model . . . \n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(train_dataset.shape[1], input_dim=train_dataset.shape[1], init='normal', activation='relu'))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(Dense(1024, init='normal', activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(train_labels.shape[1], init='normal', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.5)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how to record and plot the losses after fitting.  This is accomplished through \"callbacks\".  Here, we'll define a class that will have two methods in it:  on_train_begin and on_epoch_end.  When the fit routine begins, it will make a callback to the on_train_begin method in the LossHistory class.  This initializes the variable \"losses\".  Then, at the end of each epoch, the method on_epoch_end is called, which then appends the loss value for the epoch to the \"losses\" list.  From there, we should be able to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.9882 - acc: 0.7364     \n",
      "Epoch 2/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.5331 - acc: 0.8332     \n",
      "Epoch 3/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.3791 - acc: 0.8804     \n",
      "Epoch 4/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.3107 - acc: 0.9056     \n",
      "Epoch 5/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.2558 - acc: 0.9212     \n",
      "Epoch 6/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.1772 - acc: 0.9466     \n",
      "Epoch 7/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.1538 - acc: 0.9552     \n",
      "Epoch 8/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.1313 - acc: 0.9628     \n",
      "Epoch 9/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.1353 - acc: 0.9620     \n",
      "Epoch 10/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0981 - acc: 0.9714     \n",
      "Epoch 11/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0698 - acc: 0.9796     \n",
      "Epoch 12/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0653 - acc: 0.9816     \n",
      "Epoch 13/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0538 - acc: 0.9844     \n",
      "Epoch 14/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0369 - acc: 0.9904     \n",
      "Epoch 15/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0263 - acc: 0.9922     \n",
      "Epoch 16/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0383 - acc: 0.9894     \n",
      "Epoch 17/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0184 - acc: 0.9940     \n",
      "Epoch 18/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0211 - acc: 0.9934     \n",
      "Epoch 19/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0123 - acc: 0.9968     \n",
      "Epoch 20/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0254 - acc: 0.9928     \n",
      "Epoch 21/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0234 - acc: 0.9934     \n",
      "Epoch 22/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0134 - acc: 0.9966     \n",
      "Epoch 23/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0182 - acc: 0.9948     \n",
      "Epoch 24/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0135 - acc: 0.9958     \n",
      "Epoch 25/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0090 - acc: 0.9968     \n",
      "Epoch 26/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0091 - acc: 0.9966     \n",
      "Epoch 27/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0080 - acc: 0.9968     \n",
      "Epoch 28/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0060 - acc: 0.9982     \n",
      "Epoch 29/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0060 - acc: 0.9976     \n",
      "Epoch 30/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0049 - acc: 0.9984     \n",
      "Epoch 31/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0045 - acc: 0.9986     \n",
      "Epoch 32/32\n",
      "5000/5000 [==============================] - 1s - loss: 0.0046 - acc: 0.9980     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14363a290>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create an object LossHistory()\n",
    "history = LossHistory()\n",
    "\n",
    "# Note the argument at the end of the fit method \"callbacks = [history]\"\n",
    "model.fit(train_dataset[:train_subset, :], train_labels[:train_subset], nb_epoch=32, batch_size=32, verbose=1, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x144491a50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wPluO/mXeniaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14363add0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(32), history.losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18720/18724 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_dataset, test_labels, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is 90.760521\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score is %f' % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
